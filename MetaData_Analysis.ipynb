{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Website Builder - Create a Free Website Today | Wix.com\n",
      "Description: Everything you need to create your website, your way. From an intuitive website builder to built-in hosting and business solutions—Try Wix for free.\n",
      "Canonical: https://www.wix.com\n",
      "Favicon: https://www.wix.com/favicon.ico\n",
      "H1: Create a website without limits\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_website_details(url):\n",
    "    try:\n",
    "        # Ensure the URL starts with 'http://' or 'https://'\n",
    "        if not url.startswith((\"http://\", \"https://\")):\n",
    "            url = f\"https://{url}\"\n",
    "\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=30)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (e.g., 404)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Scrape the details\n",
    "        title = soup.title.string if soup.title else \"No Title Found\"\n",
    "        description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = description[\"content\"] if description else \"No Description Found\"\n",
    "        canonical = soup.find(\"link\", attrs={\"rel\": \"canonical\"})\n",
    "        canonical = canonical[\"href\"] if canonical else \"No Canonical URL Found\"\n",
    "        favicon = soup.find(\"link\", attrs={\"rel\": \"icon\"}) or soup.find(\"link\", attrs={\"rel\": \"shortcut icon\"})\n",
    "        favicon = favicon[\"href\"] if favicon else \"No Favicon Found\"\n",
    "        h1 = soup.find(\"h1\")\n",
    "        h1 = h1.text.strip() if h1 else \"No H1 Found\"\n",
    "\n",
    "        # Return the results\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"canonical\": canonical,\n",
    "            \"favicon\": favicon,\n",
    "            \"h1\": h1,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "url = \"wix.com\"  # Now you can just enter 'wix.com'\n",
    "details = scrape_website_details(url)\n",
    "\n",
    "# Print the results\n",
    "for key, value in details.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python programming': {'count': 0, 'density': 0.0}, 'python': {'count': 1, 'density': 0.28}, 'programming': {'count': 1, 'density': 0.28}}\n",
      "waheed iqbal search this site embedded files skip to main content skip to navigation waheed iqbal home courses publications students projects waheed iqbal   home welcome! i am an associate professor (tenured) in the department of data science, faculty of computing & information technology (ex-pucit), university of the punjab, lahore pakistan.  i have completed my ph.d., from the  asian institute of technology , master of engineering in computer science from the  asian institute of technology , and master of information technology from the  barcelona school of informatics, spain .   i have  contributed  in opennebula as part of google summer of code 2010. recent updates sept 2022:   [paper]  accepted  paper  in  ieee systems journal july 2021:    [paper]  accepted  paper  in  ieee internet of things oct 2020:     [paper]  accepted  paper  in  ieee transactions on network and service management june 2020:   [paper]  accepted  paper  in  ieee systems journal may 2020:    [paper]  accepted  paper  in  ieee transactions on service computing may 2020:    [organizing committee]  serving workshop and special session chair for  smart data 2020   jan 2020:     [track chair]  serving track chair for computing in societal automation of  sac2020 oct 2019:     [paper]  accepted  paper  in  future generation computer systems oct 2019:     [paper]  accepted  paper  in  cloudcom 2019 sept 2019:   [paper]  accepted  paper  in  ieee transactions on cloud computing july 2019:    [paper]  accepted  paper  in  ieee transactions on network and service management may 2019:    [paper]  accepted  paper  in  ieee systems journal may 2019:    [paper]  accepted  paper  in  ieee transactions on service computing email:    waheed.iqbal   at   pucit.edu.pk phone:  +92-42-111923923 extension 418 office:   pucit, allama iqbal campus.  mailing address:  punjab university college of information technology allama iqbal campus, university of the punjab, the mall,  lahore, pakistan.  research interests cloud computing distributed systems big data machine learning and text mining  technical expertise private cloud infrastructure management using openstack, opennebula, and eucalyptus public cloud infrastructure management using aws and mazure large scale system performance evaluation  big data tools:  spark, hadoop, kafka, hbase, amazon elastic map reduce (emr) programming languages:  java, c, ruby, node.js, python, php machine learning tools:  weka, prediction.io, amazon machine learning, azure machine learning  google sites report abuse google sites report abuse\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url = \"https://www.waheediqbal.info\"\n",
    "try:\n",
    "    response = requests.get(url, timeout=15)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    keyword = \"python programming\"\n",
    "    # Extract body text\n",
    "    body_text = soup.get_text(separator=\" \").lower()\n",
    "    words = body_text.split()\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Analyze keyword\n",
    "    keyword_data = {}\n",
    "    keywords = [keyword.lower()] + keyword.lower().split()\n",
    "    \n",
    "    for word in keywords:\n",
    "        count = len(re.findall(rf\"\\b{re.escape(word)}\\b\", body_text))\n",
    "        density = round((count / total_words) * 100, 2) if total_words > 0 else 0\n",
    "        keyword_data[word] = {\"count\": count, \"density\": density}\n",
    "    \n",
    "    print(keyword_data)\n",
    "    #print(body_text)\n",
    "except Exception as e:\n",
    "    print({\"error\": f\"Failed to analyze {url}: {str(e)}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
